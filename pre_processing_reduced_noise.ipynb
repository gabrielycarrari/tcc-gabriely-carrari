{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrielycarrari/tcc/blob/pre-processing-reduced-noise/pre_processing_reduced_noise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook para Pré-processamento"
      ],
      "metadata": {
        "id": "6VIUkzuXJCki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importações"
      ],
      "metadata": {
        "id": "sdnfrXU5JIBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow-io\n"
      ],
      "metadata": {
        "id": "_t5mBA7oFdUL",
        "outputId": "aa800b73-6add-449b-c702-3a795d2bf3b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: tensorflow-io in /usr/local/lib/python3.10/dist-packages (0.36.0)\n",
            "Collecting tensorflow-io\n",
            "  Using cached tensorflow_io-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
            "  Using cached tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Using cached tensorflow_io-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.6 MB)\n",
            "Using cached tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "Installing collected packages: tensorflow-io-gcs-filesystem, tensorflow-io\n",
            "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
            "    Found existing installation: tensorflow-io-gcs-filesystem 0.36.0\n",
            "    Uninstalling tensorflow-io-gcs-filesystem-0.36.0:\n",
            "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.36.0\n",
            "  Attempting uninstall: tensorflow-io\n",
            "    Found existing installation: tensorflow-io 0.36.0\n",
            "    Uninstalling tensorflow-io-0.36.0:\n",
            "      Successfully uninstalled tensorflow-io-0.36.0\n",
            "Successfully installed tensorflow-io-0.37.1 tensorflow-io-gcs-filesystem-0.37.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-io==0.36.0"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cr3oV2REcar2",
        "outputId": "d451614b-7b3d-426e-d1aa-df5823e0ac60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-io==0.36.0\n",
            "  Using cached tensorflow_io-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting tensorflow-io-gcs-filesystem==0.36.0 (from tensorflow-io==0.36.0)\n",
            "  Using cached tensorflow_io_gcs_filesystem-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Using cached tensorflow_io-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.4 MB)\n",
            "Using cached tensorflow_io_gcs_filesystem-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "Installing collected packages: tensorflow-io-gcs-filesystem, tensorflow-io\n",
            "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
            "    Found existing installation: tensorflow-io-gcs-filesystem 0.37.1\n",
            "    Uninstalling tensorflow-io-gcs-filesystem-0.37.1:\n",
            "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.37.1\n",
            "  Attempting uninstall: tensorflow-io\n",
            "    Found existing installation: tensorflow-io 0.37.1\n",
            "    Uninstalling tensorflow-io-0.37.1:\n",
            "      Successfully uninstalled tensorflow-io-0.37.1\n",
            "Successfully installed tensorflow-io-0.36.0 tensorflow-io-gcs-filesystem-0.36.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_io as tfio"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pBrGJodR80Fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qre6v-btJXZs",
        "outputId": "34de1da6-d1e0-4bf3-f81a-d330c0083d1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer"
      ],
      "metadata": {
        "id": "sUVmwIIHhXyY",
        "outputId": "df8062ca-60c3-4aa6-d076-83e6d8dcafad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.10/dist-packages (3.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.7)\n",
            "Requirement already satisfied: rapidfuzz<4,>=3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (3.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install noisereduce"
      ],
      "metadata": {
        "id": "7okhLrsh0szV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.models import load_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from jiwer import wer\n"
      ],
      "metadata": {
        "id": "n3TSHVRGNtjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olebgUsl08B0"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "login(token=userdata.get('HUGGINGFACE_TOKEN'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fm-9CjUoJcgK",
        "outputId": "ebc8cc21-5ed0-4af7-81d5-55ddb09a8079"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_model = '/content/drive/MyDrive/Models/'"
      ],
      "metadata": {
        "id": "AQbwscAPmU5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "l22eY00mNYZ1",
        "outputId": "c0152050-a5dd-4138-fd7a-897343e6bf90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o dataset Common Voice 17\n",
        "dataset = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"pt\", split='train', trust_remote_code=True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OnD5x-t_Jr6Z",
        "outputId": "5bd5d364-5c56-463f-9099-240f1c015865",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o dataset Common Voice 17\n",
        "dataset_validation = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"pt\", split='validation', trust_remote_code=True)"
      ],
      "metadata": {
        "id": "FKjEoH3NFW73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definindo Caracteres"
      ],
      "metadata": {
        "id": "DdNOyJjBKyy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The set of characters accepted in the transcription.\n",
        "characters = [x for x in \"abcdefghijklmnopqrstuvwxyzáàâãéêíóôõúç'?!.- \"]\n",
        "\n",
        "# Mapping characters to integers\n",
        "char_to_num = keras.layers.StringLookup(vocabulary=characters, oov_token=\"\")\n",
        "\n",
        "# Mapping integers back to original characters\n",
        "num_to_char = keras.layers.StringLookup(\n",
        "    vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"The vocabulary is: {char_to_num.get_vocabulary()} \"\n",
        "    f\"(size={char_to_num.vocabulary_size()})\"\n",
        ")"
      ],
      "metadata": {
        "id": "r_VSvCsMKzXx",
        "outputId": "dd8b7436-f929-40f9-d529-850ef69ae2ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary is: ['', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'á', 'à', 'â', 'ã', 'é', 'ê', 'í', 'ó', 'ô', 'õ', 'ú', 'ç', \"'\", '?', '!', '.', '-', ' '] (size=45)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "hBwR9hcROUSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The window length in samples.\n",
        "frame_length = 256\n",
        "# The number of samples to step.\n",
        "frame_step = 160\n",
        "# The size of the FFT to apply.\n",
        "fft_length = 384"
      ],
      "metadata": {
        "id": "ZRppM8pQOT7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_single_sample(audio_path, label):\n",
        "  ###########################################\n",
        "  ##  Process the Audio\n",
        "  ##########################################\n",
        "  # 1. Read wav file\n",
        "  file = tf.io.read_file(audio_path)\n",
        "  # 2. Decode the wav file\n",
        "  audio = tfio.audio.decode_mp3(file)\n",
        "  audio = tf.squeeze(audio, axis=-1)\n",
        "  # 3. Change type to float\n",
        "  audio = tf.cast(audio, tf.float32)\n",
        "\n",
        "  # 4. Reduzir o ruído usando noisereduce via tf.numpy_function\n",
        "  audio = tf.numpy_function(nr.reduce_noise, [audio, 48000], tf.float32)\n",
        "\n",
        "  # 2. Convert audio to a tensor\n",
        "  audio = tf.convert_to_tensor(audio, dtype=tf.float32)\n",
        "\n",
        "  # 4. Get the spectrogram\n",
        "  spectrogram = tf.signal.stft(\n",
        "      audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length\n",
        "  )\n",
        "  # 5. We only need the magnitude, which can be derived by applying tf.abs\n",
        "  spectrogram = tf.abs(spectrogram)\n",
        "  spectrogram = tf.math.pow(spectrogram, 0.5)\n",
        "  # 6. normalisation\n",
        "  means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n",
        "  stddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n",
        "  spectrogram = (spectrogram - means) / (stddevs + 1e-10)\n",
        "\n",
        "\n",
        "  ###########################################\n",
        "  ##  Process the label\n",
        "  ##########################################\n",
        "  # 7. Convert label to Lower case\n",
        "  label = tf.strings.lower(label)\n",
        "  # 8. Split the label\n",
        "  label = tf.strings.unicode_split(label, input_encoding=\"UTF-8\")\n",
        "  # 9. Map the characters in label to numbers\n",
        "  label = char_to_num(label)\n",
        "  # 10. Return a dict as our model is expecting two inputs\n",
        "  return spectrogram, label"
      ],
      "metadata": {
        "id": "Nvm95dLjTt14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "# Define the trainig dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (list(dataset[\"path\"]), list(dataset[\"sentence\"]))\n",
        ")"
      ],
      "metadata": {
        "id": "kC3P8ktNDjS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = (\n",
        "    train_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .padded_batch(batch_size)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "CzQPr6wnEcEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the validation dataset\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (list(dataset_validation[\"path\"]), list(dataset_validation[\"sentence\"]))\n",
        ")\n",
        "validation_dataset = (\n",
        "    validation_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .padded_batch(batch_size)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "08ruHoHDE0Lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Model Architecture"
      ],
      "metadata": {
        "id": "96KCkEum_zJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CTCLoss(y_true, y_pred):\n",
        "    # Compute the training-time loss value\n",
        "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
        "    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
        "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
        "\n",
        "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "\n",
        "    loss = keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "6sJsf0Mi_wAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_dim, output_dim, rnn_layers=5, rnn_units=128):\n",
        "    \"\"\"Model similar to DeepSpeech2.\"\"\"\n",
        "    # Model's input\n",
        "    input_spectrogram = layers.Input((None, input_dim), name=\"input\")\n",
        "    # Expand the dimension to use 2D CNN.\n",
        "    x = layers.Reshape((-1, input_dim, 1), name=\"expand_dim\")(input_spectrogram)\n",
        "    # Convolution layer 1\n",
        "    x = layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=[11, 41],\n",
        "        strides=[2, 2],\n",
        "        padding=\"same\",\n",
        "        use_bias=False,\n",
        "        name=\"conv_1\",\n",
        "    )(x)\n",
        "    x = layers.BatchNormalization(name=\"conv_1_bn\")(x)\n",
        "    x = layers.ReLU(name=\"conv_1_relu\")(x)\n",
        "    # Convolution layer 2\n",
        "    x = layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=[11, 21],\n",
        "        strides=[1, 2],\n",
        "        padding=\"same\",\n",
        "        use_bias=False,\n",
        "        name=\"conv_2\",\n",
        "    )(x)\n",
        "    x = layers.BatchNormalization(name=\"conv_2_bn\")(x)\n",
        "    x = layers.ReLU(name=\"conv_2_relu\")(x)\n",
        "    # Reshape the resulted volume to feed the RNNs layers\n",
        "    x = layers.Reshape((-1, x.shape[-2] * x.shape[-1]))(x)\n",
        "    # RNN layers\n",
        "    for i in range(1, rnn_layers + 1):\n",
        "        recurrent = layers.GRU(\n",
        "            units=rnn_units,\n",
        "            activation=\"tanh\",\n",
        "            recurrent_activation=\"sigmoid\",\n",
        "            use_bias=True,\n",
        "            return_sequences=True,\n",
        "            reset_after=True,\n",
        "            name=f\"gru_{i}\",\n",
        "        )\n",
        "        x = layers.Bidirectional(\n",
        "            recurrent, name=f\"bidirectional_{i}\", merge_mode=\"concat\"\n",
        "        )(x)\n",
        "        if i < rnn_layers:\n",
        "            x = layers.Dropout(rate=0.5)(x)\n",
        "    # Dense layer\n",
        "    x = layers.Dense(units=rnn_units * 2, name=\"dense_1\")(x)\n",
        "    x = layers.ReLU(name=\"dense_1_relu\")(x)\n",
        "    x = layers.Dropout(rate=0.5)(x)\n",
        "    # Classification layer\n",
        "    output = layers.Dense(units=output_dim + 1, activation=\"softmax\")(x)\n",
        "    # Model\n",
        "    model = keras.Model(input_spectrogram, output, name=\"DeepSpeech_2\")\n",
        "    # Optimizer\n",
        "    opt = keras.optimizers.Adam(learning_rate=1e-4)\n",
        "    # Compile the model and return\n",
        "    model.compile(optimizer=opt, loss=CTCLoss)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Get the model\n",
        "model = build_model(\n",
        "    input_dim=fft_length // 2 + 1,\n",
        "    output_dim=char_to_num.vocabulary_size(),\n",
        "    rnn_units=512,\n",
        ")\n",
        "model.summary(line_length=110)"
      ],
      "metadata": {
        "id": "DHcmgKDN_yJP",
        "outputId": "5f0fc031-7bfc-425b-fddc-0fc5ec5c58c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"DeepSpeech_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"DeepSpeech_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m            Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input (\u001b[38;5;33mInputLayer\u001b[0m)                             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m193\u001b[0m)                   │                   \u001b[38;5;34m0\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ expand_dim (\u001b[38;5;33mReshape\u001b[0m)                           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m193\u001b[0m, \u001b[38;5;34m1\u001b[0m)                │                   \u001b[38;5;34m0\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ conv_1 (\u001b[38;5;33mConv2D\u001b[0m)                                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m97\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │              \u001b[38;5;34m14,432\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ conv_1_bn (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m97\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │                 \u001b[38;5;34m128\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ conv_1_relu (\u001b[38;5;33mReLU\u001b[0m)                             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m97\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │                   \u001b[38;5;34m0\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ conv_2 (\u001b[38;5;33mConv2D\u001b[0m)                                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │             \u001b[38;5;34m236,544\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ conv_2_bn (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │                 \u001b[38;5;34m128\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ conv_2_relu (\u001b[38;5;33mReLU\u001b[0m)                             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │                   \u001b[38;5;34m0\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ reshape (\u001b[38;5;33mReshape\u001b[0m)                              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1568\u001b[0m)                  │                   \u001b[38;5;34m0\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                  │           \u001b[38;5;34m6,395,904\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                  │                   \u001b[38;5;34m0\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                  │           \u001b[38;5;34m4,724,736\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                  │                   \u001b[38;5;34m0\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ bidirectional_3 (\u001b[38;5;33mBidirectional\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                  │           \u001b[38;5;34m4,724,736\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                  │                   \u001b[38;5;34m0\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ bidirectional_4 (\u001b[38;5;33mBidirectional\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                  │           \u001b[38;5;34m4,724,736\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                  │                   \u001b[38;5;34m0\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ bidirectional_5 (\u001b[38;5;33mBidirectional\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                  │           \u001b[38;5;34m4,724,736\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                  │           \u001b[38;5;34m1,049,600\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dense_1_relu (\u001b[38;5;33mReLU\u001b[0m)                            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                  │                   \u001b[38;5;34m0\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)                            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                  │                   \u001b[38;5;34m0\u001b[0m │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m)                    │              \u001b[38;5;34m47,150\u001b[0m │\n",
              "└────────────────────────────────────────────────┴─────────────────────────────────────┴─────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                                   </span>┃<span style=\"font-weight: bold\"> Output Shape                        </span>┃<span style=\"font-weight: bold\">             Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">193</span>)                   │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ expand_dim (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)                           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">193</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ conv_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │              <span style=\"color: #00af00; text-decoration-color: #00af00\">14,432</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ conv_1_bn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ conv_1_relu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ conv_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │             <span style=\"color: #00af00; text-decoration-color: #00af00\">236,544</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ conv_2_bn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │                 <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ conv_2_relu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)                              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1568</span>)                  │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">6,395,904</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                  │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,724,736</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                  │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ bidirectional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,724,736</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                  │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ bidirectional_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,724,736</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                  │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ bidirectional_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,724,736</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,600</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dense_1_relu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                  │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                  │                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├────────────────────────────────────────────────┼─────────────────────────────────────┼─────────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>)                    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">47,150</span> │\n",
              "└────────────────────────────────────────────────┴─────────────────────────────────────┴─────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m26,642,830\u001b[0m (101.63 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,642,830</span> (101.63 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m26,642,702\u001b[0m (101.63 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,642,702</span> (101.63 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m128\u001b[0m (512.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> (512.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A utility function to decode the output of the network\n",
        "def decode_batch_predictions(pred):\n",
        "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
        "    # Use greedy search. For complex tasks, you can use beam search\n",
        "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0]\n",
        "    # Iterate over the results and get back the text\n",
        "    output_text = []\n",
        "    for result in results:\n",
        "        result = tf.strings.reduce_join(num_to_char(result)).numpy().decode(\"utf-8\")\n",
        "        output_text.append(result)\n",
        "    return output_text\n",
        "\n",
        "\n",
        "# A callback class to output a few transcriptions during training\n",
        "class CallbackEval(keras.callbacks.Callback):\n",
        "    \"\"\"Displays a batch of outputs after every epoch.\"\"\"\n",
        "\n",
        "    def __init__(self, dataset):\n",
        "        super().__init__()\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def on_epoch_end(self, epoch: int, logs=None):\n",
        "        predictions = []\n",
        "        targets = []\n",
        "        for batch in self.dataset:\n",
        "            X, y = batch\n",
        "            batch_predictions = model.predict(X)\n",
        "            batch_predictions = decode_batch_predictions(batch_predictions)\n",
        "            predictions.extend(batch_predictions)\n",
        "            for label in y:\n",
        "                label = (\n",
        "                    tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n",
        "                )\n",
        "                targets.append(label)\n",
        "        wer_score = wer(targets, predictions)\n",
        "        print(\"-\" * 100)\n",
        "        print(f\"Word Error Rate: {wer_score:.4f}\")\n",
        "        print(\"-\" * 100)\n",
        "        for i in np.random.randint(0, len(predictions), 2):\n",
        "            print(f\"Target    : {targets[i]}\")\n",
        "            print(f\"Prediction: {predictions[i]}\")\n",
        "            print(\"-\" * 100)"
      ],
      "metadata": {
        "id": "mVQyGCLO_3nF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "E05uwiEb_7QW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Época 1"
      ],
      "metadata": {
        "id": "jHKs9fxatMJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of epochs.\n",
        "epochs = 10\n",
        "# Callback function to check transcription on the val set.\n",
        "validation_callback = CallbackEval(validation_dataset)\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=epochs,\n",
        "    callbacks=[validation_callback],\n",
        ")"
      ],
      "metadata": {
        "id": "-C8mQup-_7AN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Saving Model\")\n",
        "model.save(path_model + 'model_reduced_v1.keras')"
      ],
      "metadata": {
        "id": "0c8eEl-D2Krf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Época 4"
      ],
      "metadata": {
        "id": "vWyPXqDpsueb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(path_model + 'model_reduced_v1.keras', custom_objects={'CTCLoss': CTCLoss})"
      ],
      "metadata": {
        "id": "ltNUDEIWsxwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of epochs.\n",
        "epochs = 1\n",
        "# Callback function to check transcription on the val set.\n",
        "validation_callback = CallbackEval(validation_dataset)\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=epochs,\n",
        "    callbacks=[validation_callback],\n",
        ")"
      ],
      "metadata": {
        "id": "7ZNxgeVLst3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Saving Model\")\n",
        "model.save(path_model + 'model_reduced_v2.keras')"
      ],
      "metadata": {
        "id": "K2ZDlXPtuQDT",
        "outputId": "08300e64-c034-4a00-d4cb-54ca1fd39e6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Model\n"
          ]
        }
      ]
    }
  ]
}